# Lakehouse Dev

[![Python](https://img.shields.io/badge/python-3.12-blue)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/pyspark-4.0-orange)](https://spark.apache.org/)
[![Delta Lake](https://img.shields.io/badge/delta-lake-007ec6)](https://delta.io/)
[![Airflow](https://img.shields.io/badge/airflow-3.0-red)](https://airflow.apache.org/)

Projeto de referÃªncia para arquitetura **Data Lakehouse**, com pipelines de ingestÃ£o, transformaÃ§Ã£o e orquestraÃ§Ã£o, utilizando **Docker** para facilitar o desenvolvimento e testes locais.

O projeto adota camadas de dados tÃ­picas do Lakehouse: **bronze â†’ silver â†’ gold**, separando ingestÃ£o, limpeza e modelagem.

---

## Tecnologias utilizadas

* **Python 3.12** â€“ linguagem principal para pipelines e scripts
* **PySpark 4** â€“ processamento distribuÃ­do de dados
* **Delta Tables 4** â€“ armazenamento transacional, versionado e otimizado para queries
* **MinIO** â€“ armazenamento compatÃ­vel com S3, usado como data lake local
* **Airflow 3** â€“ orquestraÃ§Ã£o e scheduling de pipelines
* **Selenium** â€“ coleta automatizada de dados da web

---

Segue a versÃ£o revisada, com linguagem mais clara, tÃ©cnica e profissional:

---

## Estrutura de Pastas e Arquitetura do Projeto

A organizaÃ§Ã£o do projeto foi definida com foco em **separaÃ§Ã£o de responsabilidades**, **manutenibilidade**, **escalabilidade** e **clareza arquitetural**.

Cada diretÃ³rio possui um papel bem definido dentro da arquitetura Lakehouse adotada.

### Estrutura de DiretÃ³rios

```txt
lakehouse_dev/
â”‚
â”œâ”€â”€ README.md                  # DocumentaÃ§Ã£o principal do projeto
â”œâ”€â”€ requirements.txt           # DependÃªncias Python adicionais
â”œâ”€â”€ pyproject.toml             # ConfiguraÃ§Ãµes e metadados do projeto
â”œâ”€â”€ Makefile                   # AutomaÃ§Ã£o de build e execuÃ§Ã£o
â”œâ”€â”€ compose.yaml               # OrquestraÃ§Ã£o dos containers Docker
â”œâ”€â”€ .env                       # VariÃ¡veis de ambiente e credenciais
â”‚
â”œâ”€â”€ core/                      # MÃ³dulos e utilitÃ¡rios compartilhados
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ spark_session.py   # ConfiguraÃ§Ã£o centralizada da SparkSession
â”‚
â”œâ”€â”€ data_contracts/            # DefiniÃ§Ã£o de contratos e validaÃ§Ãµes de dados
â”‚
â”œâ”€â”€ airflow/                   # ConfiguraÃ§Ãµes e estrutura do Airflow
â”‚   â”œâ”€â”€ dags/                  # DefiniÃ§Ã£o das DAGs
â”‚   â”œâ”€â”€ configs/               # ConfiguraÃ§Ãµes especÃ­ficas
â”‚   â””â”€â”€ plugins/               # Plugins customizados
â”‚
â”œâ”€â”€ pipelines/                 # LÃ³gica de ingestÃ£o e transformaÃ§Ã£o
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ gold/
â”‚   â””â”€â”€ config/                # ConfiguraÃ§Ãµes especÃ­ficas de pipelines
â”‚
â”œâ”€â”€ crawlers/                  # AutomaÃ§Ãµes e coleta de dados externos
â”‚   â””â”€â”€ selenium/              # ImplementaÃ§Ãµes com Selenium
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â””â”€â”€ Dockerfile             # Imagem base (Airflow 3 + Spark 4 + Delta Lake)
â”‚
â”œâ”€â”€ scripts/                   # Scripts auxiliares
â””â”€â”€ tests/                     # Testes unitÃ¡rios e de integraÃ§Ã£o
```

---

## OrganizaÃ§Ã£o Arquitetural

A arquitetura segue o padrÃ£o **Data Lakehouse**, com separaÃ§Ã£o clara entre:

* **OrquestraÃ§Ã£o** (Airflow)
* **Processamento** (Spark + Delta Lake)
* **Armazenamento** (MinIO)
* **IngestÃ£o externa** (Crawlers)
* **GovernanÃ§a e qualidade** (Data Contracts)

Essa estrutura permite evoluÃ§Ã£o modular, testes isolados e adaptaÃ§Ã£o futura para ambientes em cloud.

---

## DescriÃ§Ã£o dos Principais Componentes

### `requirements.txt`

Gerencia dependÃªncias Python adicionais ao ambiente base da imagem Docker.
Sempre que novas bibliotecas forem adicionadas, Ã© necessÃ¡rio **reconstruir a imagem Docker** para refletir as mudanÃ§as no ambiente.

---

### `Makefile`

Centraliza comandos recorrentes do projeto (build, start, stop, logs, etc.).
Reduz complexidade operacional e padroniza a execuÃ§Ã£o do ambiente.

---

### `.env`

ResponsÃ¡vel pela definiÃ§Ã£o de:

* Credenciais
* VariÃ¡veis de ambiente
* ConfiguraÃ§Ãµes sensÃ­veis

Esse arquivo **nÃ£o deve ser versionado** em ambientes produtivos.

---

### `core/`

ContÃ©m mÃ³dulos reutilizÃ¡veis e utilitÃ¡rios compartilhados entre pipelines e DAGs.
A centralizaÃ§Ã£o evita duplicaÃ§Ã£o de cÃ³digo e melhora a padronizaÃ§Ã£o tÃ©cnica.

---

### `data_contracts/`

Destinado Ã  definiÃ§Ã£o de contratos de dados, schemas esperados e validaÃ§Ãµes.
Tem como objetivo garantir:

* Qualidade
* ConsistÃªncia
* GovernanÃ§a dos dados

---

### `airflow/`

ContÃ©m todos os artefatos relacionados Ã  orquestraÃ§Ã£o:

* `dags/` â†’ definiÃ§Ã£o dos fluxos de execuÃ§Ã£o
* `configs/` â†’ configuraÃ§Ãµes especÃ­ficas
* `plugins/` â†’ extensÃµes customizadas

Essa estrutura mantÃ©m o Airflow desacoplado da lÃ³gica de negÃ³cio.

---

### `pipelines/`

ContÃ©m a lÃ³gica de ingestÃ£o e transformaÃ§Ã£o organizada nas camadas:

* **bronze/** â†’ ingestÃ£o bruta (raw data)
* **silver/** â†’ dados tratados e enriquecidos
* **gold/** â†’ dados prontos para consumo analÃ­tico

O diretÃ³rio Ã© montado dentro do container do Airflow, garantindo que qualquer novo desenvolvimento seja automaticamente reconhecido pelas DAGs.

---

### `crawlers/`

Destinado exclusivamente Ã  coleta de dados externos.

A separaÃ§Ã£o entre `pipelines` e `crawlers` foi uma decisÃ£o arquitetural para:

* Isolar responsabilidades
* Facilitar manutenÃ§Ã£o
* Permitir futura substituiÃ§Ã£o da estratÃ©gia de ingestÃ£o

Esse diretÃ³rio tambÃ©m Ã© montado dentro do container do Airflow.

---

### `docker/`

ContÃ©m os artefatos necessÃ¡rios para construÃ§Ã£o da imagem personalizada do projeto, baseada em:

* Airflow 3
* Spark 4
* Delta Lake

Essa abordagem garante reprodutibilidade e padronizaÃ§Ã£o do ambiente.

---

### `scripts/`

Armazena scripts auxiliares que nÃ£o fazem parte diretamente dos pipelines, mas apoiam o desenvolvimento ou operaÃ§Ã£o.

---

### `tests/`

DiretÃ³rio dedicado a testes unitÃ¡rios e de integraÃ§Ã£o.
Permite validaÃ§Ã£o contÃ­nua da lÃ³gica de transformaÃ§Ã£o e regras de negÃ³cio.

---

## BenefÃ­cios da Estrutura Adotada

* SeparaÃ§Ã£o clara de responsabilidades
* ModularizaÃ§Ã£o da lÃ³gica de negÃ³cio
* Facilidade de testes e manutenÃ§Ã£o
* Ambiente reprodutÃ­vel via Docker
* Preparado para migraÃ§Ã£o futura para ambientes em cloud

---

## PrÃ©-requisitos

Antes de executar o projeto, vocÃª precisa ter instalado:

* Docker
* Docker Compose
* Git

---

## InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1 - Clone o repositÃ³rio

```bash
git clone git@github.com:Alberto-Oliveira-Barbosa/lakehouse_dev.git
cd lakehouse_dev
```

### 2 - Configure o ambiente

```bash
cp .env.example .env
```

Edite o `.env` conforme necessÃ¡rio.

---

## Executando com Docker

### Subir todos os serviÃ§os

```bash
make up
```

ou

```bash
docker compose up --build -d
```

### Parar serviÃ§os

```bash
make down
```

### ğŸ“œ Logs

Logs gerais:

```bash
make logs
```

Logs do Airflow:

```bash
make logs_airflow
```

Logs do MinIO:

```bash
make logs_minio
```

---

## ConfiguraÃ§Ãµes ObrigatÃ³rias Antes de Rodar as DAGs

---

### Senha automÃ¡tica no Apache Airflow 3

A partir da versÃ£o 3, o Airflow **gera automaticamente a senha do usuÃ¡rio admin na inicializaÃ§Ã£o do container**.

Ela **nÃ£o Ã© mais fixa**.

Para obter a senha:

```bash
make logs_airflow
```

ou

```bash
docker compose logs airflow
```

Procure nos logs por algo como:

```
lakehouse-airflow  | Simple auth manager | Password for user 'admin': UsAxMF67F86Wh3Dw

```

Acesse a interface:

```
http://localhost:8081
```

UsuÃ¡rio padrÃ£o:

```
admin
```

Sem recuperar essa senha nos logs, nÃ£o serÃ¡ possÃ­vel acessar o painel.

---

### CriaÃ§Ã£o do Bucket no MinIO

Antes de executar as DAGs de exemplo, Ã© **obrigatÃ³rio criar manualmente o bucket no MinIO**.

### Acessar o MinIO

DisponÃ­vel em:

```
http://localhost:9001
```

Use as credenciais definidas no `.env`:

```
MINIO_ROOT_USER
MINIO_ROOT_PASSWORD
```

### Criar o Bucket

1. Acesse o console do MinIO
2. Clique em **Buckets**
3. Selecione **Create Bucket**
4. Crie o bucket com o nome esperado pelas DAGs (por default esse template espera ao menos um Bucket com o nome  `lakehouse`, demais camadas ou sub-diretÃ³rios ele consegue gerar na escrita.)

âš ï¸ Caso o bucket nÃ£o exista, as DAGs irÃ£o falhar ao tentar gravar dados.
---

## Executando as DAGs

ApÃ³s:

* Subir os containers
* Recuperar a senha do Airflow
* Criar o bucket no MinIO

Acesse o Airflow:

```
http://localhost:8081
```

Ative e execute as DAGs disponÃ­veis.
